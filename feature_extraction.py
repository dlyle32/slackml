import osimport jsonimport nltkfrom nltk.corpus import stopwordsimport reimport pandas as pdimport numpy as npimport mathfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfVectorizerwpt = nltk.WordPunctTokenizer()stop_words = stopwords.words('english')def normalize_document(doc):    # lower case and remove special characters\whitespaces    doc = re.sub(r'[^a-zA-Z\s]', '', doc, re.I | re.A)    doc = doc.lower()    doc = doc.strip()    # tokenize document    tokens = wpt.tokenize(doc)    # filter stopwords out of document    filtered_tokens = [token for token in tokens if token not in stop_words]    # re-create document from filtered tokens    doc = ' '.join(filtered_tokens)    return docdef build_word_matrix(events):    corpus = [event.get("text") for event in events]    corpus = np.array(corpus)    normalize_corpus = np.vectorize(normalize_document)    norm_corpus = normalize_corpus(corpus)    tfidf_vectorizer = TfidfVectorizer(use_idf=True)    tfidf_vectors = tfidf_vectorizer.fit_transform(norm_corpus)    # cv = CountVectorizer()    # cv_matrix = cv.fit_transform(norm_corpus)    # vocab = cv.get_feature_names()    # corpus_df = pd.DataFrame(cv_matrix.toarray(), columns=vocab)    corpus_df = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names())    return corpus_dfdef encode_channels(channels):    channelSeries = pd.Series(channels, name="Channel")    return pd.get_dummies(channelSeries, dtype=float)def encode_users(users):    userSeries = pd.Series(users, name="User")    return pd.get_dummies(userSeries)def extract_features_to_file(events, users, channels, batch_number):    corpus_df = build_word_matrix(events)    channel_encoding = encode_channels(channels)    users = encode_users(users)    features = corpus_df.join(channel_encoding, lsuffix='_caller', rsuffix='_other')    features.to_csv("/Users/davidlyle/slack_ml/features/feats_"+str(batch_number)+".csv")    print("features written for batch " + str(batch_number))def is_event_valid(event):    is_user_message = event.get("type") == "message" and event.get("user") and event.get("text");    words = event.get("text").split(" ") if event.get("text") != None else []    return is_user_message and len(words) > 2def process_file(fname):    with open(fname,'r') as fp:        data = json.load(fp)    events = []    users = []    for event in data:        if is_event_valid(event):            events.append(event)            users.append(event["user"])    return events, usersdef process_file_batch(files, batch_number):    events = []    users = []    channels = []    for file in files:        channel = file.split('/')[-2]        file_events, file_users = process_file(file)        events.extend(file_events)        users.extend(file_users)        channels.extend([channel for evt in file_events])    extract_features_to_file(events, users, channels, batch_number)def load_data(slack_messages_dir, batch_size=100):    files = []    for r, d, f in os.walk(slack_messages_dir):        for file in f:            files.append(os.path.join(r,file))    num_batches = math.floor(len(files)/20)    for i in range(0,num_batches):        process_file_batch(files[i*batch_size: (i+1) * batch_size], i)    if len(files) % batch_size != 0:        process_file_batch(files[i*batch_size: len(files)], batch_size)if __name__ == "__main__":    slack_messages_dir = "/Users/davidlyle/slack_ml/slack_messages/"    load_data(slack_messages_dir)